{"cells":[{"cell_type":"markdown","source":["#CO-Following on GitHub\n#### MIE1512 \\ Jay Patel"],"metadata":{}},{"cell_type":"markdown","source":["## PREFACE\n\nThis project is based on a paper: Co-Following on Twitter By Venkata Rama Kiran Garimella and Ingmar Weber in 2014 in whcih they have presented a comprehensive study of co-following relationship in social network users, perticularly on Twitter.\n\nThe primary hypothesis is that Two twitter users whose followers have similar other followings (except those two users) are also similar. By using co-following users as feature vectors, the analysis shows the classification on binary preference and similarities in different users based on their follower's behaviour.\n\nHere, We are using the same concept of Co-Following on different social media platform: GitHub.\n\nThe notebook is in following order.\n\n1. Introduction\n2. Importing Data\n3. Data Wrangling\n4. Data Preparation\n5. Rationale of Features (Classification)\n6. Main Analysis (PCA)\n7. Interpretation of Results\n8. Conclusion\n9. Future Possible Work\n10. References"],"metadata":{}},{"cell_type":"markdown","source":["## 1. INTRODUCTION\nIn paper on co-following on Twitter, Garimella and Weber have shown the coprehensive study on co-following relationship in social network based on Twitter users' data. They showed that how this observation contributes to (1) user classification on Twitter (2) eliciting opportunities for computational social science and (3) improving online marketing by identifying cross-selling opportunitues.\n\nThey started with classification of user based on co-following data to show how accurately we can predict one user's preference on following one from two organizations. Which also paved the way for the further analysis as the accuracy of classification shows nothing else but significance of co-following data.\n\nAfter that they have found the cosine similarity among the organizations based on their co-following features and then mapped them into 2D plot with the help of Multidimensional Scaling (MDS).\nTheir result showed how two or more fundamentally different organizations falls near to each other (i.e. seemed similar) even though they don't share any explicit connection."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import PCA\nfrom pyspark.sql.functions import lit\nfrom pyspark.mllib.classification import SVMWithSGD, SVMModel\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import SparseVector\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\nfrom pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\nfrom pyspark.sql import DataFrame\n\n#non-spark library\nfrom functools import reduce\nimport numpy as np\nimport matplotlib.pyplot as plt"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def vectorizer(dataframe, inputCol, outputCol, vocabSize, minDF):\n    '''\n    function that converts the data to vectors in N space.\n    INPUT - DATAFRAME, INPUT COLUMN (in which you have your list of user-data you want to convert into  Vectors)\n            OUTPUT COLUMN (under what name you want to save Vectors in DATAFRAME),\n            vocabSie - max size of vectors desired (recommended-2000 max for one cluster),\n            minDF - min number of times the data should occur for it to be consider as a vector.\n    OUTPUT - new DATAFRAME with added column of vectors\n    '''\n    \n    #vectorizing the co-follower features\n    vector = CountVectorizer(inputCol=inputCol, outputCol=outputCol,vocabSize=vocabSize, minDF=minDF)\n    model = vector.fit(dataframe)\n\n    df_featureLabel = model.transform(dataframe)\n    #df_featureLabel.show(truncate=True)\n    \n    return df_featureLabel\n\ndef getPCA(dataframe, inputCol, Ndimension):\n    '''\n    Function for Dimension Reduction\n    Use PCA - Principal Component Analysis\n    INPUT - DATAFRAME with feature vector column.\n            Ndimension (number of dimension you want to reduce your data into) (Ndimension = 2 for this project)\n            inputCol (name of the input dataframe column with high dimensional data)\n    OUTPUT - new dataframe with column 'pcaFeature'\n    '''\n    pca = PCA(k=Ndimension, inputCol=inputCol, outputCol=\"pcaFeatures\")\n    model = pca.fit(dataframe)\n    return model.transform(dataframe)\n\ndef getLabelPoint(datatframe):\n    '''\n    Function for creating LabeledPoints\n    INPUT - Dataframe with column names: 'label' and 'features'\n    OUTPUT - RDD of LabeledPoints\n    '''\n  \n    #creating LabeledPoint vectors from features and label columns \n    #dataframe to RDD\n    df_to_rdd = datatframe.rdd.map(lambda y: (y.label, y.features))\n    \n    #labeledPoints\n    labeledVectors = df_to_rdd.map(lambda y: LabeledPoint(y[0], MLLibVectors.fromML(y[1])))\n    \n    return labeledVectors\n  \ndef Classification(labeledVectors):\n    '''\n    Function for Classification with SVM\n    INPUT - RDD of LabeledPoint\n    OUTPUT - [AUC-ROC, AUC-PR] type:list\n    '''\n    #splitting hte data into train and test \n    splits = labeledVectors.randomSplit([0.6, 0.4], 1234)\n    train = splits[0]\n    test = splits[1]\n    # Build the model\n    model = SVMWithSGD.train(train)\n\n    # Evaluating the model on training data\n    labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n    #trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(labeledVectors.count())\n    #print(\"Training Error = \" + str(trainErr))\n    predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n\n    metrics = BinaryClassificationMetrics(predictionAndLabels)\n\n    predictions = model.predict(test)\n    #print(\"Area under PR = %s\" % metrics.areaUnderPR)\n    #print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n    \n    return [metrics.areaUnderROC, metrics.areaUnderPR]\n  \n#combining all three collection of organizations\n\ndef unionAll(*dfs):\n    '''\n    function derived from https://stackoverflow.com/questions/33743978/spark-union-of-multiple-rdds\n    takes all the dataframes you want to concate as arguments and produce a giant dtaframe \n    only obvious condition is all the input dataframes must have same number of columns with same name. \n    '''\n    return reduce(DataFrame.unionAll, dfs)\n  "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["##2. IMPORTING DATA"],"metadata":{}},{"cell_type":"markdown","source":["#### 2.1 IMPORTING DATASETS AND SAVING INTO LOCAL DIRECTORY\n\nThe dataset used here is from GHTorrent which is the largest offline mirror data from GitHub REST API which can be accessible through Google BigQuery interface. The latest SQL dump by the time of the project was from April 2018. The dataset imported here is from GHTorrent April 2018 mySQL dump.\n\nGoogle account and credentials used for BigQuery: Shaily Patel (Team Member)"],"metadata":{}},{"cell_type":"code","source":["%sh wget -P/FileStore/tables https://storage.googleapis.com/157watcher/organizations1.csv"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sh wget -P/FileStore/tables https://storage.googleapis.com/157watcher/followers1.csv"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#explore the local directory\ndbutils.fs.ls(\"file:/FileStore/tables/\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["####2.2 CREATING SPARK DATAFRAMES\nFrom the imported data in form of CSV, now we convert those data to spark readable format: <i>pyspark.sql.dataframe.DataFrame<i>\n\nWe also specify the schema manually as spark can not encode the timestamp in proper datatype from the BigQuery."],"metadata":{}},{"cell_type":"code","source":["#building Schema\norganizationsSchema = StructType([StructField('org_id', StringType(), True, \n                                             {'description': 'unique organization id', 'type': 'integer'}), \n                                 StructField('user_id', StringType(), True, \n                                             {'description': 'unique member(user) id', 'type': 'integer'}), \n                                 StructField('created_at', TimestampType(), True, \n                                             {'description': 'when the user added as member', 'type':'timestamp',\\\n                                              'warning':'the date recorded if GHTorrent has recorded corresponding \\\n                                              event, otherwise the date will correspond to users or organizations \\\n                                              joining date.'})])\n\nfollowersSchema = StructType([StructField('user_id', StringType(), True, \n                                             {'description': 'unique organization id', 'type': 'integer'}), \n                                 StructField('follower_id', StringType(), True, \n                                             {'description': 'unique member(user) id', 'type': 'integer'}), \n                                 StructField('created_at', TimestampType(), True, \n                                             {'description': 'when the user started following the follower_id',\\\n                                              'type':'timestamp', 'warning':'the date recorded if GHTorrent has \\\n                                              recorded corresponding event, otherwise the date will correspond \\\n                                              to users or followers joining date.'})])"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["organizations = spark.read.csv('file:/FileStore/tables/organizations1.csv',header=True, schema=organizationsSchema)\nfollowers = spark.read.csv('file:/FileStore/tables/followers1.csv', header=True, schema=followersSchema)\norganizations.printSchema()\nfollowers.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["####2.3 Creating temporary table views for running query with magic SQL"],"metadata":{}},{"cell_type":"code","source":["organizations.createOrReplaceTempView(\"organizations\")\nfollowers.createOrReplaceTempView(\"followers\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["##3. DATA-WRANGLING"],"metadata":{}},{"cell_type":"markdown","source":["####3.1 Basic Questions to access Structure"],"metadata":{}},{"cell_type":"markdown","source":["<b>Q1. Do all records in the dataset contain the same fields?</b>\n\nSince the Schema is set manually,\n1. dataset: organizations, contains (1) org_id -> string (2) user_id -> string (3) created_at -> Date\n2. dataset: followers, contains (1) user_id -> string (2) follower_id -> string (3) created_at -> Date\n\n<b>Q2. How can you access the same fields across records? By position? By name?</b>\n\nSince the datasets are CSV type with the apprpriate headers, we can access the same field by position as well as by column name.\n\n<b>Q3. How are the records delimited/separated in the dataset? Do you need sophosticated parsing logic  to separate the records form one another?</b>\n\nDatasets procured from GHTorrent is readily available in form of CSV. So there was no need for parsing the data except to convert IDs to string in schema. Because, integer value might generate wrong graphs while querying. Moreover, the timestamp coversion is stated in section 3.3.1\n\n<b>Q4. How are record fields encoded? Human readable strings? Binary numbers? Hash keys? Compressed? Enumerated codes?</b>\n\nAll the records are encoded as integer and timestamp(for created_at field). All the fields are human readable.\n\n<b>Q5. What is the complexity of the encoding? Primitives elements like integers, decimal numbers, short strings and so on? Higher-order elements like key-value sets or array?</b>\n\nThe time component on dataset was troublesome in original form. Google BigQuery originally save timestamp in a certain format which is not recognised with spark SQL directly.\n\nopen issue on github: https://github.com/GoogleCloudPlatform/google-cloud-go/issues/942\n\nHowever, there is a BigQuery function, brought to attention by group member <i>Mohammed Bubshait</i> convert the datetime format to spark recognizable format. For example, to acquire the organization table, the following query was required:\n\n```sql\nSELECT  \n  org_id,\n  user_id,  \n  STRFTIME_UTC_USEC(created_at, '%Y-%m-%d %H:%M:%S') AS created_at  \nFROM [ghtorrent-bq:ght_2018_04_01.organization_members]\n```\n\n\n<b>Q7. What are the reletionship types between records and the record fields? Singular (record should have one and only one value for a field, like customer date of birth)? Set-based (record could have many values for the field, like customer shipping addresses)?</b>\n\nMost of the data in both dataset shows one to many relation between `org_id` to `user_id` (as there could be more than one member for an onrganization) and between `user_id` to `follower_id` (as a user could have followed more than one user) in `organizations` and `followers` respectively. Datasets represent these relationship as flat key-value pairs."],"metadata":{}},{"cell_type":"markdown","source":["####3.2 Basic Questions to assess Data granularity"],"metadata":{}},{"cell_type":"markdown","source":["<b>3.2.1. What kind of thing (person, object, relationship, event, etc.) do the records represent?</b>"],"metadata":{}},{"cell_type":"code","source":["%sql\n/* 2.1 DATASET - organizations*/\nSELECT * FROM organizations"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%sql\n/* 2.1 DATASET - followers */\nSELECT * FROM followers"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%sql\ndesc organizations"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%sql\ndesc followers"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["The dataset `organizations` provides the unique ID of organization `org_id`, User ID of members of organizations `user_id` and the timestamp `created_at` of when user is added to the organization as member `created_at`.\n\nThe dataset `followers` provides the user ID `user_id` and users' ID who is followed by user in first column `follower_id` and the timestamp `created_at` of when user under `user_id` has started following user under `follower_id`."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.2.2. Are the records homogeneous (represent the same kinds of things)? Or heterogeneous?</b>\n\nBoth datasets consist of Homogenenous records of being either strings ( `user_id`, `follower_id`, `org_id` ) or timestamp ( `created_at` ) in every column."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.2.3. What alternative interpretations of the records are there? For example, if the records appear to be customers, could they actually be all known contacts (only some of which are customers)?</b>\n\nDatasets used for the project has very less dimensions and there is no significant alternative interpretation. However, for dataset `followers`, one can easily interpret inverse that users in `follower_id` has followers in `user_id`."],"metadata":{}},{"cell_type":"markdown","source":["####3.3 Basic Questions to Access Data Accuracy"],"metadata":{}},{"cell_type":"markdown","source":["<b>3.3.1. Accuracy regarding datatypes </b>\n\nBoth dataset `organizations` and `followers` consist of organizations IDs and user IDs. Although github treats both organization and user differently in terms of social features, the ID given to every account (either organization or user) is on same serial.\nGHTorrent also treat users and organizations with same serial of IDs. \nREST API provides some attributes such as `type` which differentiate those two from each other. GHTorrent's `user` table provide similar attribute.\nOverall, all the ID, whether it is organization or user, both are unique big integers which are saved in table and converted to string while defining schema for easy of vectorizing in later part.\n\nThe time component on dataset was troublesome in original form. Google BigQuery originally save timestamp in a certain format which is not recognised with spark SQL directly. For more detail, see section 3.1 Q5\n\nSecondly, the data is machine generated and stored which means there is no question of human error in saving the data."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.3.2 Anomalies in Dataset</b>"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*), MONTH(created_at) FROM organizations GROUP BY MONTH(created_at) ORDER BY MONTH(created_at)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Monthly detection of membership-event is not very evenly distributed. This could be due to the fact that Organizations can toggle the privacy settings for allows everyone to see the members."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*), MONTH(created_at) FROM organizations WHERE YEAR(created_at) < 2014 GROUP BY MONTH(created_at) ORDER BY MONTH(created_at)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Above query on `organizations` dataset shows that before 2014, the event are totally missing from month 2,3,4,5,6,7  and August showed very abnormal grown in event detection."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*), MONTH(created_at) FROM followers GROUP BY MONTH(created_at)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Monthly detection of follow-event shows uniform distribution throughout."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*), created_at FROM followers  GROUP BY created_at having count(*) > 100000"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Above query on `followers` dataset shows that only 5 perticular timestamps have recorded abnormal growth of more than 100000 follow-event throughout the dataset. Which might be the missing events in GHTorrent which were then replaced by the creation date of user or follower, whichever is the latest."],"metadata":{}},{"cell_type":"markdown","source":["#### 3.4 Basic Questions to Access Data Temporality"],"metadata":{}},{"cell_type":"markdown","source":["<b>3.4.1. When was the dataset collected?</b>\n\nDataset imported from the Google BigQuery is SQLdump of GHTorrent dated: `1st April, 2018`."],"metadata":{}},{"cell_type":"code","source":["%sql\n/*1.1 DATASET - organizations*/\nSELECT * FROM organizations ORDER BY created_at DESC"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%sql\n/*1.1 DATASET - followers*/\nSELECT * FROM followers ORDER BY created_at DESC"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Result shows that latest dataset is from the date 30th March, 2018. Which also supported by the dataset name in Google BigQuery as `ght_2018_04_01`"],"metadata":{}},{"cell_type":"markdown","source":["<b>3.4.2 Were all the records and record fields collected/measured at the same time? If not, is the temporal range significant?</b>\n\n>The `created_at` field is only filled in accurately for memberships for which GHTorrent has recorded a corresponding event. Otherwise, it is filled in with the latest date that the corresponding user or organization has been created. <i>source: http://ghtorrent.org/relational.html</i>\n\nIn short, data is recorded as happened in real-time as best as possible."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT created_at, count(*) FROM organizations GROUP BY created_at HAVING count(*) > 200 "],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Above query on `organizations` shows the ONLY THREE days and perticular TIME when more than 200 membership-event has been recorded. Which can be supported as above quoted GHTorrent's method of providing creation date of organizations or member instead of original membership-event timestamp in case of missing events."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.4.3 Are the timestamps associated with collection of the data known and available (as a record field) or as associated metadata? </b>\n\nThere is no metadata available on when the data is collected. But, each dump has the end date of the data collection."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.4.4 Have some records or record field values been modified after the time of creation? Are the timestamps of these modification available?</b>\n\nFor the dataset `organizations`, time of organization and any user get together in organization-member connection is recorded as that time of event. However, there could be some inaccuracy with the idnividual request to the data from REST API which is confirmed by Gousios as he has observed that some REST API calls return slightly modified results if htey queried in different time moments.[2] \n\nSecondly, an organization has control over whether to display members publicaly or not to. Therefore, the data we get is only from those organizations which has given the access to.\n\nIn the dataset `followers`, again, the data is recorded as it happened in time. However, there are some possibility of data inconsistency such as: user account get deleted, user stop following previously followed user, etc."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.4.5 In what ways can you determine if the data is \"stale\"? </b>\n\nWe can use the GitHub REST API manually via python library such as pygithub to check whether the `org_id` in sample corresponds to an active organization as well as whether a `user_id` is been deleted or diactivated currently. We can query random sample using `RAND()` function and generate a list of IDs to check."],"metadata":{}},{"cell_type":"markdown","source":["#### 3.5 Baic Questions to Access Data Scope"],"metadata":{}},{"cell_type":"markdown","source":["<b>3.5.1 Given the granularity of the dataset, what characteristics of the things represented by the records are captured by the record fields? What characteristics are not captured? </b>\n\nThe dataset, or perticularly, the tables from GHTorrent datasets: `organizations` and `followers` are very shallow in terms of granularity since both datasets have the detail such as: the organizations and their memebr users and event-time of user being member of organization; the user and followings and event-time of user started following other. There is no details such as user status in organization or total contribution in repositories owned by organizations."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.5.2 Are the record fields consistent? </b>\n\n<b>3.5.3 Do the records in the dataset represent the entire population of the associated things?</b>\n\nSince the GHTorrent have made it clear that the dataset has only points (i.e. `org_id` and `user_id`) which has information publically available and GHTorrent do not capture (even as `null` point) those `org_id` or `user_id` which have privacy restrictions."],"metadata":{}},{"cell_type":"markdown","source":["<b>3.5.4 Are there missing racords? Is it randomly missing or systematically missing?</b>\n\nGHTorrent declares that the event of being memebr of an organization and user following other user has been captured as they occur. However, due to malfunctioning in the mirroring system (software or network) could result in some part of data that are missing. This missing event data can be restored if it is still in newest 300 events per repository(this limit is imposed by GitHub REST API). Any most frequently changing project could have more than 300 events per day which means the missing events of those repositories can not be restored.[2]"],"metadata":{}},{"cell_type":"markdown","source":["##4. Co-Following on GitHub"],"metadata":{}},{"cell_type":"markdown","source":["####4.1 VARIATION OF CO-FOLLOWING ON GITHUB FOR ORGANIZATIONS\nGitHub is fundamentaly different from the Twitter. In twitter, there is no difference between an individual user such as @tim_cook and an organization such as @Apple. These both are similar in terms of their behavious in following and followed by others However, GitHub has explicit difference between a user and an organization. In GitHub, there could be a user who can follow other users and followed by other users as well but an organization can not follow other users or organizations and vise versa. Organization in GitHub can have members(which are individual users) and these members has to be invited by the organization itself for membership. One user could be member of several organizations.\n\nSince GitHub is a social media for collaborative work especially in the field of software engineering, there are hardly any individuals who has solely gained popularity. It is always a combine effort from a group of people (random group of contributors for a pertiuclar project or members of an organization) for making their project famous on the platform.\n\nHere, I am proposing two different methods derived from original co-Following method of twitter to look into the similarity among organizations in the GitHub: \n\n<b>(1) CoMembership : </b> For an organization in GitHub, the members of which could also be member of other organizations which are called CoMemberships of that organization. For example, if a user @dev is one of the member of organization @team and @dev also a member of other organizations such as @org1, @org2, and @org3; then @org1, @org2, and @org3 are coMemberships of @team with respect to @dev.\n\n<b>(2) CoFollowing : </b>As discussed earlier, organizations don't have privilage to follow other organizations or users, but their members have certain followship as they are individual account. So coFollowing of an organizations are the users' account which are followerd by the members of that organizations. For example, if user @dev follows other users such as @user1, @user2, and @user3; then @user1, @user2, and @user3 are called coFollowings of the organization @team with respect to @dev. Note that here we are assuming that being member of an organization is similar to follow it to get similar model as that of twitter.\n\n\n<b>NOTE</b> After November 2015, Organizations can select whether membership information is revealed to external parties. That means we are only analysing the organizations which have not made membership information private."],"metadata":{}},{"cell_type":"markdown","source":["####4.2 Example of Data\n\nTable 1 shows the organizations ID in first column and their members' ID in second column. It is one to many relationship since one organization could have more than one users as member.\nFor example, Table 1 shows organization <i>A</i> has user <i>a,b,c,d,e</i> as members. Note that we use the word <em>root organization</em> as organizations on which we are performing analysis.\n\nTable 1: Root organization and its members\n\nroot organization | member \n--- | --- \nA | a\nA | b\nA | c\nA | d\nA | e\nB | c\nB | d\nB | f\nC | g\nC | t\nC | y\n\nTable 2 shows the users and their membership to respective organizations. It is again one to many relationship but reverse from table 1 since one user could be member of multiple organizations simulteneously.\nFor example, table 2 shows user <i>a</i> has membership in organizations <i>A,F,G</i>, and <i>X</i> with <i>A</i> being root organization. Every user in this table must have at least one root organization.\n\nTable 2: user and its organization\n\nuser | organiztion \n--- | --- \na | A\na | F\na | G\na | X\nb | A\nb | D\nb | F\nc | W\nc | A\nc | B\nd | A\nd | B\nd | R\ne | A\nf | B\nf | H\ng | C\ng | R\nt | C\nt | G\nt | H\n\nTable 3 shows the user and their followers. It is one to many relationship as one user can follow more than one users.\n\nTable 3: user and their follower\n\nuser | follwoer \n--- | --- \na | q\na | w\na | s\na | z\nb | v\nb | d\nb | f\nb | g\nc | v\nc | f\nc | r\nd | w\nd | e\nd | f\ne | q\ne | a\nf | g\nf | v\nf | e\nf | d\ng | y\ng | h\ng | f\nt | e\nt | r\nt | y\ny | h\ny | g\ny | a\n\n\n\n#### 4.2.1 co-membership Data\n\nNow we join the organization to organizations on one to many relationship based on the member users of root organizations. The first column in table 4 shows root organizations and second column are all other organization connected to root organization with relationship of co-membership. For example, as seen in above table 1 and 2, root organization <i>C</i> has three members: <i>g,t</i>, and <i>y</i> who are also members of organizations <i>C,R,G,H</i> cummulatively which we called as co-membership of root organization. So <i>R,H,G</i> are co-membership of root organization <i>C</i>. Note that we remove <i>C</i> from co-membership as it is obvious that organiztions can not be counted as its own co-member. \n\nTable 4: Root Organization and their co-members\n\nroot organization | co-member \n--- | --- \nA | F\nA | G\nA | X\nA | D\nA | W\nA | B\nA | R\nB | A\nB | W\nB | R\nB | H\nC | R\nC | G\nC | H\n\n\nFinally, table 5 shows grouped co-members of every root organization.\n\nTable 5: Root organization and their grouped co-members\n\nroot organization | co-member\n--- | ---\nA | [F, G, X, D, W, B, R]\nB | [A, W, R, H]\nC | [R, G, H]\n\n\n#### 4.2.2 co-following Data\n\nNow we join the organization to users on one to many relationship based on the member users of root organizations. The first column in table 6 shows root organizations and second column are all followers of the members of root organization. For example, as seen in above table 1 and 3, root organization <i>C</i> has three members: <i>g,t</i>, and <i>y</i> who follows <i>y,h,f,e,r,g</i>, and <i>a</i> cummulatively which we called as co-followings of root organization. So <i>y,h,f,e,r,g</i>, and <i>a</i> are co-membership of root organization <i>C</i>. Note that we <u>do not</u> remove <i>y</i> from co-following here as opposed to co-membership as <i>y</i> is coming from the following of <i>g</i> and <i>t</i> which is fair. \n\nTable 6: Root Organization and their co-members\n\nroot organization | co-following \n--- | --- \nA | q\nA | w\nA | s\nA | z\nA | v\nA | d\nA | f\nA | g\nA | r\nA | w\nA | e\nA | a\nB | v\nB | f\nB | r\nB | w\nB | e\nB | d\nC | y\nC | h\nC | f\nC | e\nC | r\nC | g\nC | a\n\n\nFinally, table 7 shows grouped co-following of every root organization.\n\nTable 4: Root organization and their grouped co-following\n\nroot organization | co-following\n--- | ---\nA | [q, w, s, z, v, d, f, g, r, w, e, a]\nB | [v, f, r, w, e, d]\nC | [y, h, f, e, r, g, a]"],"metadata":{}},{"cell_type":"markdown","source":["#### 4.2.3 Feature Matrix\n\nWe use this co-member as feature vector with countVectorizer from MLlib.\nLet's consider all co-member in dataset as n-dimesion vector.\nFor AllCoMembers = [A, B, D, F, G, H, R, W, X]\n\nSo the feature matrix would look like this:\n\nORG | A | B | D | F | G | H | R | W | X\n--- | --- | --- | --- | --- | --- | --- | --- | --- \nA | 0 | 1 | 1 | 1 | 1 | 0 | 1 | 1 | 1\nB | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0\nC | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0\n\nNote: Similar structure is obtained for co-follwer analysis."],"metadata":{}},{"cell_type":"markdown","source":["##BIG QUESTION | do co-membership and co-following really matters?\nBefore we start using co-membership and co-following data as feature data for further analysis,  we ask ourselves whether co-membership and co-following data really give significant information about the organizations. To answer this question, we ask another more specific question that, for co-membership data, given the users and which organizations they possess membership of, can we predict if certain user be member of any perticular organization and for co-following data, given users and their followers, can we predict whether a user be member of a certain organization. This will be answered using classification algorithm (SVM) for evaluation."],"metadata":{}},{"cell_type":"markdown","source":["##5. Rationale of Features (Binary user Classification)\n\n####BIG QUESTION - do co-membership and co-following really matters?\nBefore we start using co-membership and co-following data as feature data for further analysis,  we ask ourselves whether co-membership and co-following data really give significant information about the organizations. To answer this question, we ask another more specific question that, for co-membership data, given the users and which organizations they possess membership of, can we predict if certain user be member of any perticular organization and for co-following data, given users and their followers, can we predict whether a user be member of a certain organization. This will be answered using classification algorithm (SVM) on organization pairs for evaluation."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT \n  org_id,\n  COUNT(user_id) as totalMember\nFROM \n  organizations\nGROUP BY org_id\nORDER BY totalMember DESC\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["#### 5.1 Steps for Labeled Data\n1. Select two organizations\n2. Get two dataframe, one for each organization as `org_id` in first column and  `coMembership` in second.\n3. Group all co-member on organization as described above.\n4. Add one more column in both dataframes that act as label: 1 for dataframe1 and 0 for dataframe2 <strong>*(see `for loop` in code below for data processing decribed in step 1 to 4)*</strong>\n5. Use columns `labels` and `features` as input for creating RDD of LabeledPoints. <strong>*(see function `getLabelPoint`)*</strong>\n\n#### 5.2 Steps for finding Classification accuracy\n\n1. Input - labeledData for co-membership data ** *(from above mentioned steps)* **\n2. **For** every-possible pair of root organizations **do** Binary Classification \n3. Evaluate accuracy for each pair\n4. Find average accuaracy\n\nsame steps repeated for coFollowing data as well.\n\n*NOTE: following code takes pair of organizations and do classification over co-following data as well as co-membership data(another loop). The loop is doing data preprocessing initially and then use functions such as vectorizer, classification, etc. which defined above. However, the loop has very long runtime so here we just select 3 organizations (with nearly 600 members in each) to get classification result for each pair (total 2^3 = 8 pairs) which takes 3 minutes(8 times 3 = 24 minutes for each loop which is approximately 1 hr for both loops) of time. I delibarately choose smaller list of organization to run through quickly. As mentioned earlier, classification is just to make sure that co-following and co-membership data really have significance over organizations characteristics. We also limits the vector size to 1000 and minDF=2 *\n\n*NOTE: in PCA, every feature vector is at size of 2000 as the partition value for single cluster in Databricks spark is 2000 and for vector size more than 2000 will exceed the java heap memory and cause error*"],"metadata":{}},{"cell_type":"markdown","source":["Cell below shows the code for creating arbitary list of organizations (here 3 organizations with each having number of members approximately 600)\n`for loop` used to make pairs of organzations for classification afterward."],"metadata":{}},{"cell_type":"code","source":["top5_org = [259105, 1007812, 7022203] #selected from top organization query as they have nearly 600 members in each.\npair_of_orgs = []\nfor i in top5_org:\n  for j in top5_org:\n    if i != j:\n      if (j,i) not in pair_of_orgs:\n          pair_of_orgs.append((i,j))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["####5.3 Classification on Co-members\nCells below show the `for loop` that measure classification accuracy for every possible pairs of given organizations and at the end gives average accuracy."],"metadata":{}},{"cell_type":"code","source":["accuracy_of_coMembership = []\n\nfor num, i in enumerate(pair_of_orgs):\n  \n    #creating 2 dataframe each for 2 organizations\n    #sql query for acquiring co-member data for given organizations\n    df1 = spark.sql('''SELECT M.org_id as org, M.user_id as member, N.org_id as coMember FROM\n                      (SELECT org_id, user_id FROM organizations WHERE org_id = {0}) AS M JOIN\n                      organizations AS N ON M.user_id = N.user_id\n                      WHERE N.org_id != {1}'''.format(i[0],i[0]))\n    df2 = spark.sql('''SELECT M.org_id as org, M.user_id as member, N.org_id as coMember FROM\n                      (SELECT org_id, user_id FROM organizations WHERE org_id = {0}) AS M JOIN\n                      organizations AS N ON M.user_id = N.user_id\n                      WHERE N.org_id != {1}'''.format(i[1],i[1]))\n    \n    #dataframe to rdd\n    df1_ = df1.rdd.map(lambda p: (p[1], p[2]))\n    df2_ = df2.rdd.map(lambda p: (p[1], p[2]))\n    \n    #convert flat key value list to list of value for each key\n    df1tuple__ = df1_.groupByKey().mapValues(list)\n    df2tuple__ = df2_.groupByKey().mapValues(list)\n\n    #removing dublicate values if any\n    df1tuple = df1tuple__.map(lambda p: (p[0],list(set(p[1]))))\n    df2tuple = df2tuple__.map(lambda p: (p[0],list(set(p[1]))))\n    \n    #removing common members of both dataset \n    df1tuple_ = df1tuple.subtractByKey(df2tuple)\n    df2tuple_ = df2tuple.subtractByKey(df1tuple)\n    \n    #creating spark dataframes with third column added as label with value of 1 and 0 for respective dataframes\n    dataframe1 = spark.createDataFrame(df1tuple_, ['member', 'coMember']).withColumn('label', lit(0))\n    dataframe2 = spark.createDataFrame(df2tuple_, ['member', 'coMember']).withColumn('label', lit(1))\n    \n    #join two dataframes\n    dataframe = dataframe1.unionAll(dataframe2)\n    \n    #create new column in dataframe for features vectors\n    dataframeVec = vectorizer(dataframe, 'coMember', 'features', 1000, 10)\n    \n    labelDF = getLabelPoint(dataframeVec)\n    \n    classification_matrix = Classification(labelDF)\n    \n    accuracy_of_coMembership.append(classification_matrix[0]) #put 1 for getting average accuracy based on PR, 0 for ROC\n    \n    print(\"loop \", num, \"complete. Remained \", len(pair_of_orgs) - num)\n    \navg_coMember_accuracy = np.mean(accuracy_of_coMembership)\nprint(\"The average co-membership classification accuracy is :\", avg_coMember_accuracy)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["####5.4 Classification on Co-following\nCells below show the `for loop` that measure classification accuracy for every possible pairs of given organizations and at the end gives average accuracy."],"metadata":{}},{"cell_type":"code","source":["\naccuracy_for_coFollowing = []\nfor num, i in enumerate(pair_of_orgs):\n  \n    #creating 2 dataframe each for 2 organizations\n    df1 = spark.sql('''SELECT A.org_id, A.user_id, C.follower_id FROM organizations AS A JOIN followers AS C ON A.user_id = C.user_id WHERE A.org_id = {0}'''.format(i[0]))\n    df2 = spark.sql('''SELECT A.org_id, A.user_id, C.follower_id FROM organizations AS A JOIN followers AS C ON A.user_id = C.user_id WHERE A.org_id = {0}'''.format(i[1]))\n    \n    #dataframe to rdd\n    df1_ = df1.rdd.map(lambda p: (p[1], p[2]))\n    df2_ = df2.rdd.map(lambda p: (p[1], p[2]))\n    \n    #convert flat key value list to list of value for each key\n    df1tuple__ = df1_.groupByKey().mapValues(list)\n    df2tuple__ = df2_.groupByKey().mapValues(list)\n\n    #removing dublicate values if any\n    df1tuple = df1tuple__.map(lambda p: (p[0],list(set(p[1]))))\n    df2tuple = df2tuple__.map(lambda p: (p[0],list(set(p[1]))))\n\n    #removing common members from both dataset \n    df1tuple_ = df1tuple.subtractByKey(df2tuple)\n    df2tuple_ = df2tuple.subtractByKey(df1tuple)\n    \n    #creating spark dataframes with third column added as label with value of 1 and 0 for respective dataframes\n    dataframe1 = spark.createDataFrame(df1tuple_, ['org_id', 'follower_id']).withColumn('label', lit(0))\n    dataframe2 = spark.createDataFrame(df2tuple_, ['org_id', 'follower_id']).withColumn('label', lit(1))\n    \n    #join two dataframes\n    dataframe = dataframe1.unionAll(dataframe2)\n    \n    #create new column in dataframe for features vectors\n    dataframeVec = vectorizer(dataframe, 'follower_id', 'features', 1000, 2)\n    \n    #convert feature vector and label column to labeledPoints RDD\n    labelDF = getLabelPoint(dataframeVec)\n    \n    #classification\n    classification_matrix = Classification(labelDF)\n    \n    \n    accuracy_for_coFollowing.append(classification_matrix[0]) #put 1 for getting average accuracy based on PR, 0 for ROC\n\n    print(\"loop \", num, \"complete. Remained \", len(pair_of_orgs) - num)\n    \navg_cofollowing_accuracy = np.mean(accuracy_for_coFollowing)\nprint(\"The average co-membership classification accuracy is :\", avg_cofollowing_accuracy)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["####5.5 Classification Result Interpretation"],"metadata":{}},{"cell_type":"markdown","source":["##6. Main Analysis (PCA)\n\n####6.1 Procedure for PCA (Principal Component Analysis)\n1. Feature vectors for co-membership and co-following are user as N-dimensional data.\n2. Implement PCA (from the spark MLlib) to reduce dimensionality of feature vectors"],"metadata":{}},{"cell_type":"markdown","source":["###PCA on Co-membership in Different sets of organizations"],"metadata":{}},{"cell_type":"markdown","source":["####6.2 CoMembership Similarity in Top 10 Organizations"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT \n  org_id,\n  COUNT(user_id) as totalMember\nFROM \n  organizations\nGROUP BY org_id\nORDER BY totalMember DESC\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["%sql\n/*To avoid over-complicated query, the organizations IDs are given manually*/\n/*this query also makes sure that we dont collect same organizations for its coMembers*/\nSELECT M.org_id as org, M.user_id as member, N.org_id as coMember FROM\n(SELECT org_id, user_id FROM organizations WHERE org_id IN (3886902, 95143, 3681780, 3815374, 3432832, 2206, 259105, 1007812, 7022203, 2156)) AS M JOIN\norganizations AS N ON M.user_id = N.user_id\nWHERE N.org_id not in (3886902, 95143, 3681780, 3815374, 3432832, 2206, 259105, 1007812, 7022203, 2156)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["##### Description for Code in Below\n1. Dataframe generation from sql query\n2. creating RDD from dataframe using only column 1(organization id) and 3(co-membership) ** *Note: co-following would be 3rd column in co-following analysis* **\n3. Grouped list from Flat-value RDD i.e. from `org_id: comember1, org_id: comember2,...` to `org_id : [comember1, comember2, ...]` \n4. Removing Dublicated from grouped list\n5. creating dataframe with two columns: `organizations` and `features`\n6. creating feature vectors from `feature` column *(see function **Vectorizer**)*\n7. PCA on feature vectors.\n\n**Note: This code get repeated for different set of organizations analysis**"],"metadata":{}},{"cell_type":"code","source":["#spark dataframe for above stated query\ncoMembership_top10 = spark.sql(\n'''\nSELECT M.org_id as org, M.user_id as member, N.org_id as coMember FROM\n(SELECT org_id, user_id FROM organizations WHERE org_id IN (3886902, 95143, 3681780, 3815374, 3432832, 2206, 259105, 1007812, 7022203, 2156)) AS M JOIN\norganizations AS N ON M.user_id = N.user_id\nWHERE N.org_id not in (3886902, 95143, 3681780, 3815374, 3432832, 2206, 259105, 1007812, 7022203, 2156)\n'''\n)\n\n#dataframe to RDD in form of tuples like: (organization, coMember)\nrdd = coMembership_top10.rdd.map(lambda p: (str(p[0]), str(p[2])))\n\n#convert flat key value list to list of value for each key\ngroupRDD = rdd.groupByKey().mapValues(list)\n\n#removing dublicate values if any\ngroupRDD_ = groupRDD.map(lambda p: (p[0],list(set(p[1]))))\n\n#RDD back to Dataframe\nDF = spark.createDataFrame(groupRDD_ , ['organizations', 'features'])\n\n#vectorizing the data\n#parameter given: Above stated Dataframe, name of input column, name for output column, max number of dimension desired, min number of time a datapoint should appear in organiations to consider as vector\nDF2 = vectorizer(DF, 'features', 'vectors', 2000, 5)\n\n#dimension reduction of dataset\nDF3 = getPCA(DF2, 'vectors', 2)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["####6.2.1 Visualization using Matplotlib"],"metadata":{}},{"cell_type":"code","source":["#x and y values of data in list form\nX_top10CoMember = DF3.rdd.map(lambda p: p['pcaFeatures'][0]).collect()\nY_top10CoMember = DF3.rdd.map(lambda p: p['pcaFeatures'][1]).collect()\n\n#organization names in a list\ntop10CoMember = DF3.rdd.map(lambda p: p[0]).collect()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["fig, axs = plt.subplots()\nplt.title(\"Co-Membership on Top 10 Organizations\")\naxs.scatter(X_top10CoMember,Y_top10CoMember)\nfor i, txt in enumerate(top10CoMember):\n    axs.annotate(txt, (X_top10CoMember[i],Y_top10CoMember[i]))\ndisplay()"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["####6.2.2 Plot Interpretation\n\n+ id `3432832` i.e. `fossasia` and id `3681780` i.e. `Azure` are clustured together here. Which shows similarity in those two. Potential reason has been found that,in 2017, Microsoft was GOLD SPONSOR at FOSSASIA Summit. Which was an event for developers, startups and students using <b> open source tools </b> provided by Microsoft.\n+ Other than that, there is no significant similarity found in these organizations."],"metadata":{}},{"cell_type":"markdown","source":["####6.3 CoMembership Similarity in Top Organizations in GHImpact"],"metadata":{}},{"cell_type":"markdown","source":["###GH-IMPACT\nWhile searching for reasonable organizations to compare. I came across to the research work of a PhD student <a href='http://imiller.utsc.utoronto.ca/'>Ian Dennis Miller</a> at University of Toronto. \nHe has created a impact measurement of open source project based on the influence level.\n>gh-impact measures open source influence. gh-impact is based upon the stars a project receives: an account has a gh-impact score of <b>n</b> if they have <b>n projects</b> with <b>n stars</b>. Higher gh-impact scores correspond to accounts that have many well-used projects. \n\n<i> source: http://www.gh-impact.com/about/ </i>\n\n<b>top 20 most impactful organizations according to GHImpact score is as below:</b>\n\nname | org_id | gh-impact\n--- | --- | ---\ngoogle | 95143 | 185\nfacebook | 2156 | 147\napache | 13369 | 130\nMicrosoft | 38886902 | 104\nmozilla | 1146 | 95\ncodrops | 636722 | 92\ntwitter | 5092 | 88\nsquare | 8274| 79\ngooglesamples | 4438683 | 73\nNetflix | 1600 | 72\nmapbox | 29218 | 69\nspring-projects | 2395257 | 67\nthoughtbot | 10963 | 66\ngithub | 142 | 63\nangular | 159 | 61\nGoogleCloudPlatform | 1007812 | 61\nawslabs | 1904672 | 61\nyahoo | 7045 | 59\nAtom | 1110689 | 59\nopenstack | 23312 | 58\n\n<i> source: http://www.gh-impact.com/leaderboard/ </i>\n\nThe `org_id` is derived from another ghtorrent dataset table called `users` wwhich is not imported here due to its large size.\n\nHere is the query on GoogleBigQuery to produce `org_id` for given username of organization/user. Usernames provided in query are case sensitive. \n\n```sql\nSELECT id, login FROM [ghtorrent-bq:ght_2018_04_01.users] \nWHERE type='ORG' AND login IN ('google','facebook','apache','Microsoft','mozilla'\n,'codrops','twitter','square','googlesamples','Netflix','mapbox','spring-projects',\n'thoughtbot','github','angular','GoogleCloudPlatform','awslabs','yahoo','Atom',\n'openstack')\n```"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT M.org_id as org, M.user_id as member, N.org_id as coMember FROM\n(SELECT org_id, user_id FROM organizations WHERE org_id IN (7045, 1600, 636722, 5092, 95143, 3886902, 4438683, 13369, 8274, 10963, 23312, 159, 2156, 13369, 8274, 10963, 23312, 159, 2156, 142, 1146, 29218, 1110689, 1007812, 2395257, 1904672)) AS M JOIN\norganizations AS N ON M.user_id = N.user_id\nWHERE N.org_id not in (7045, 1600, 636722, 5092, 95143, 3886902, 4438683, 13369, 8274, 10963, 23312, 159, 2156, 13369, 8274, 10963, 23312, 159, 2156, 142, 1146, 29218, 1110689, 1007812, 2395257, 1904672)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["#spark dataframe for above stated query\ncoMembership_ghi = spark.sql(\n'''\nSELECT M.org_id as org, M.user_id as member, N.org_id as coMember FROM\n(SELECT org_id, user_id FROM organizations WHERE org_id IN (7045, 1600, 636722, 5092, 95143, 3886902, 4438683, 13369, 8274, 10963, 23312, 159, 2156, 13369, 8274, 10963, 23312, 159, 2156, 142, 1146, 29218, 1110689, 1007812, 2395257, 1904672)) AS M JOIN\norganizations AS N ON M.user_id = N.user_id\nWHERE N.org_id not in (7045, 1600, 636722, 5092, 95143, 3886902, 4438683, 13369, 8274, 10963, 23312, 159, 2156, 13369, 8274, 10963, 23312, 159, 2156, 142, 1146, 29218, 1110689, 1007812, 2395257, 1904672)\n'''\n)\n\n#dataframe to RDD in form of tuples like: (organization, coMember)\nrdd_ghi = coMembership_ghi.rdd.map(lambda p: (str(p[0]), str(p[2])))\n\n#convert flat key value list to list of value for each key\ngroupRDD_ghi = rdd_ghi.groupByKey().mapValues(list)\n\n#removing dublicate values if any\ngroupRDD_ghi_ = groupRDD_ghi.map(lambda p: (p[0],list(set(p[1]))))\n\n#RDD back to Dataframe\nDF_ghi = spark.createDataFrame(groupRDD_ghi_ , ['organizations', 'features'])\n\n#vectorizing the dataset\nDF2_ghi = vectorizer(DF_ghi, 'features', 'vectors', 2000, 5)\n\n#dimension reduction of dataset\nDF3_ghi = getPCA(DF2_ghi, 'vectors', 2)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["####6.3.1 Visualization using Matplotlib"],"metadata":{}},{"cell_type":"code","source":["#x and y values of data in list form\nX_ghi = DF3_ghi.rdd.map(lambda p: p['pcaFeatures'][0]).collect()\nY_ghi = DF3_ghi.rdd.map(lambda p: p['pcaFeatures'][1]).collect()\n\n#organization names in a list\nghiCoMember = DF3_ghi.rdd.map(lambda p: p[0]).collect()"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["fig2, axs2 = plt.subplots()\nplt.title(\"Co-Membership on Top organizations in GHImpact\")\naxs2.scatter(X_ghi,Y_ghi)\nfor i, txt in enumerate(ghiCoMember):\n    axs2.annotate(txt, (X_ghi[i],Y_ghi[i]))\nfig2.set_size_inches(18.5, 10.5, forward=True)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["####6.3.2 Plot Interpretatin\n+ `microsoft` (3886902) and `angular` (159) has clustered together. As Microsoft's TypeScript language is the best suitable for the Angular due to following features:\n  - Class-based Object Oriented Programming\n  - Static Typing\n  - Generics\n+ `mapbox`(29218), `square` (8274), `spring-project` (2395257), `awslabs` (1904672) and `codrops` (6367722) are clustered together showing similarity in CoMembership.\n+ Besides these, there are so many other organizations clustered together which might be the result of exclusiveness in co-membership."],"metadata":{}},{"cell_type":"markdown","source":["###PCA on Co-following in Different sets of organizations"],"metadata":{}},{"cell_type":"markdown","source":["####6.4 CoFollowing Similarity in Top 10 Organizations"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT \n  A.org_id, B.user_id, C.follower_id\nFROM\n   (\n    SELECT \n      org_id,\n      COUNT(user_id) as totalMember\n    FROM \n      organizations\n    GROUP BY org_id\n    ORDER BY totalMember DESC\n    LIMIT 10\n    ) AS A\nJOIN organizations AS B ON B.org_id = A.org_id\nJOIN followers AS C ON C.user_id = B.user_id\nORDER BY A.org_id"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["#spark dataframe for above stated query\nCF_top10 = spark.sql(\n'''\nSELECT \n  A.org_id, B.user_id, C.follower_id\nFROM\n   (\n    SELECT \n      org_id,\n      COUNT(user_id) as totalMember\n    FROM \n      organizations\n    GROUP BY org_id\n    ORDER BY totalMember DESC\n    LIMIT 10\n    ) AS A\nJOIN organizations AS B ON B.org_id = A.org_id\nJOIN followers AS C ON C.user_id = B.user_id\nORDER BY A.org_id\n'''\n)\n\n#dataframe to RDD in form of tuples like: (organization, coMember)\nrdd_CF_top10 = CF_top10.rdd.map(lambda p: (str(p[0]), str(p[2])))\n\n#convert flat key value list to list of value for each key\ngroupRDD_CF_top10 = rdd_CF_top10.groupByKey().mapValues(list)\n\n#removing dublicate values if any\ngroupRDD_CF_top10_ = groupRDD_CF_top10.map(lambda p: (p[0],list(set(p[1]))))\n\n#RDD back to Dataframe\nDF_CF_top10 = spark.createDataFrame(groupRDD_CF_top10_ , ['organizations', 'features'])\n\n#vectorizing the dataset\nDF2_CF_top10 = vectorizer(DF_CF_top10, 'features', 'vectors', 2000, 5)\n\n#dimension reduction of dataset\nDF3_CF_top10 = getPCA(DF2_CF_top10, 'vectors', 2)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["#### 6.4.1 Visualization using Matplotlib"],"metadata":{}},{"cell_type":"code","source":["#x and y values of data in list form\nX_top10CF = DF3_CF_top10.rdd.map(lambda p: p['pcaFeatures'][0]).collect()\nY_top10CF = DF3_CF_top10.rdd.map(lambda p: p['pcaFeatures'][1]).collect()\n\n#organization names in a list\ntop10CF = DF3_CF_top10.rdd.map(lambda p: p[0]).collect()"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["fig4, axs4 = plt.subplots()\nplt.title(\"Co-Following on Top 10 Organizations\")\naxs4.scatter(X_top10CF,Y_top10CF)\nfor i, txt in enumerate(top10CF):\n    axs4.annotate(txt, (X_top10CF[i],Y_top10CF[i]))\nfig4.set_size_inches(18.5, 10.5, forward=True)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":["#### 6.4.2 Plot Interpretation\n+ In co-following, `facebook` (2156), `google` (95143), `EpicGames` (3815374) and `github-beta` (7022203) looks similar as they fall very near in the plot.\n+ The top10 organizations having very high number of members compare to an average organizations, cause high number of co-following relationship as well. The reduction of this high dimensionality  often not be that accurate and we might loose some important information with noise as well."],"metadata":{}},{"cell_type":"markdown","source":["#### 6.5 CoFollowing Similarity in Top Organizations in GHImpact"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT \n  A.org_id, A.user_id, C.follower_id\nFROM\n   organizations AS A JOIN followers AS C ON A.user_id = C.user_id\nWHERE\n  A.org_id IN (7045, 1600, 636722, 5092, 95143, 3886902, 4438683, 13369, 8274, 10963, 23312, 159, 2156, 13369, 8274, 10963, 23312, 159, 2156, 142, 1146, 29218, 1110689, 1007812, 2395257, 1904672)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["#spark dataframe for above stated query\nCF_ghi = spark.sql(\n'''\nSELECT \n  A.org_id, A.user_id, C.follower_id\nFROM\n   organizations AS A JOIN followers AS C ON A.user_id = C.user_id\nWHERE\n  A.org_id IN (7045, 1600, 636722, 5092, 95143, 3886902, 4438683, 13369, 8274, 10963, 23312, 159, 2156, 13369, 8274, 10963, 23312, 159, 2156, 142, 1146, 29218, 1110689, 1007812, 2395257, 1904672)\n'''\n)\n\n#dataframe to RDD in form of tuples like: (organization, coMember)\nrdd_CF_ghi = CF_ghi.rdd.map(lambda p: (str(p[0]), str(p[2])))\n\n#convert flat key value list to list of value for each key\ngroupRDD_CF_ghi = rdd_CF_ghi.groupByKey().mapValues(list)\n\n#removing dublicate values if any\ngroupRDD_CF_ghi_ = groupRDD_CF_ghi.map(lambda p: (p[0],list(set(p[1]))))\n\n#RDD back to Dataframe\nDF_CF_ghi = spark.createDataFrame(groupRDD_CF_ghi_ , ['organizations', 'features'])\n\n#vectorizing the dataset\nDF2_CF_ghi = vectorizer(DF_CF_ghi, 'features', 'vectors', 2000, 5)\n\n#dimension reduction of dataset\nDF3_CF_ghi = getPCA(DF2_CF_ghi, 'vectors', 2)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":["#### 6.5.1 Visualization using Matplotlib"],"metadata":{}},{"cell_type":"code","source":["#x and y values of data in list form\nX_ghiCF = DF3_CF_ghi.rdd.map(lambda p: p['pcaFeatures'][0]).collect()\nY_ghiCF = DF3_CF_ghi.rdd.map(lambda p: p['pcaFeatures'][1]).collect()\n\n#organization names in a list\nghiCF = DF3_CF_ghi.rdd.map(lambda p: p[0]).collect()"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["fig5, axs5 = plt.subplots()\nplt.title(\"Co-Following on top Organizations in GHImpact\")\naxs5.scatter(X_ghiCF,Y_ghiCF)\nfor i, txt in enumerate(ghiCF):\n    axs5.annotate(txt, (X_ghiCF[i],Y_ghiCF[i]))\nfig5.set_size_inches(18.5, 10.5, forward=True)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":["####6.5.2 Plot Interpretation\n+ `codrops` (636722), `awslabs` (1904672), `googlesample` (4438683) and `spring-projects` (2395257) are clustered together.\n+ `netflix` (1600) and `angular` (159) shows more closeness than above stated ones.\n+ `mozilla` (5092) and `twitter` (1146) shows similarity."],"metadata":{}},{"cell_type":"markdown","source":["####6.6 CoFollowing Similarity in Organizations from GitHub Collections\n\n<a href='https://github.com/collections'>GitHub Collections</a> are  curated lists of similar open source projects. GitHub explore an dhand-picked this repositories. Since we are only looking into organizations. We will pickup only those repositories that are owned by an organization rather than a single user."],"metadata":{}},{"cell_type":"markdown","source":["GitHub collection: <b>Open_Journalism</b> is the collection of repositories made by famous publications and data-driven journalisms to power their newsroom and ensure informatin is reported fairly and accurately.\n\nname | org_id\n--- | --- \nvoxmedia | 508316 \nguardian | 11571 \npropublica | 86422 \nnewsapps | 93349 \nBloombergMedia | 6415567 \ndukechronicle | 2545549 \ntimes | 2771911 \ncensusreporter | 1772387\nNYTimes | 72081 \nnprapps | 247435\nfivethirtyeight | 3176682\nTimeMagazine | 2886039\n\n<i> source: https://github.com/collections/open-journalism </i>\n\nGitHub Collection: <b>Getting_started_with_machine_learning</b> is the collection of open source machine learning library and repositories of open source datasets\n\nname | org_id\n--- | --- \nscikit-learn | 23655 \naikorea | 8557654 \nTheano | 9504 \nGSA | 63708 \nnationalparkservice | 637539 \ntenserflow | 10451648 \ndeepmind | 6170681 \nGoogleTrends | 8026505\napache | 13369 \nshogun-toolbox | 41562\nsrc-d | 10092617\nopenai | 10741300\nfivethirtyeight | 2886039\n\n<i> source: https://github.com/collections/machine-learning </i>\n\nGitHub Collection: <b>Hacking Minecraft</b> is the collection of projects related to Minecraft game.\n\nname | org_id\n--- | --- \nBukkit | 9938 \ndocker | 234594 \nMovingBlocks | 4660 \noverviewer | 9321 \nessentials | 19347 \nPrismarineJS | 7115233 \nMinecraftForge | 9635 \nSpigotMC | 1983947\nmsmhq | 8742236 \nPocketMine | 1463194\nminefold | 60842\nMightyPirates | 3084255\nGlowstoneMC | 5035267\ncuberite | 6485533\n\n<i> source: https://github.com/collections/hacking-minecraft </i>\n\nBelow, we import the dataframe contains 'id' and 'username' of these all organizations."],"metadata":{}},{"cell_type":"code","source":["%sh wget -P/FileStore/tables https://www.dropbox.com/s/rq1x8e2e9cv63sa/openjournalism.csv"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["%sh wget -P/FileStore/tables https://www.dropbox.com/s/8wed5cv656zd51o/machinelearning.csv"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["%sh wget -P/FileStore/tables https://www.dropbox.com/s/wx8q18sfqcgtci5/minecraft.csv"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["machineLearning = spark.read.csv('file:/FileStore/tables/machinelearning.csv', header=True)\nopenJournalism = spark.read.csv('file:/FileStore/tables/openjournalism.csv', header=True)\nminecraft = spark.read.csv('file:/FileStore/tables/minecraft.csv', header=True)"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["#Here I am only examining machine learning and minecreaft datasets\n#combining machine learning and minecreaft datasets\ncollection = unionAll(machineLearning, minecraft)\ncollection.createOrReplaceTempView('collections')"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["%sql\nSELECT \n  A.org_id, C.login, A.user_id, F.follower_id\nFROM\n  organizations AS A JOIN followers AS F ON A.user_id = F.user_id JOIN collections AS C ON A.org_id = C.id\nWHERE A.org_id IN (SELECT id FROM collections)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["#spark dataframe for above stated query\nCF_col = spark.sql(\n'''\nSELECT \n  A.org_id, C.login, A.user_id, F.follower_id\nFROM\n  organizations AS A JOIN followers AS F ON A.user_id = F.user_id JOIN collections AS C ON A.org_id = C.id\nWHERE A.org_id IN (SELECT id FROM collections)\n'''\n)\n\n#dataframe to RDD in form of tuples like: (organization, coMember)\nrdd_CF_col = CF_col.rdd.map(lambda p: (str(p[1]), str(p[3])))\n\n#convert flat key value list to list of value for each key\ngroupRDD_CF_col = rdd_CF_col.groupByKey().mapValues(list)\n\n#removing dublicate values if any\ngroupRDD_CF_col_ = groupRDD_CF_col.map(lambda p: (p[0],list(set(p[1]))))\n\n#RDD back to Dataframe\nDF_CF_col = spark.createDataFrame(groupRDD_CF_col_ , ['organizations', 'features'])\n\n#vectorizing the dataset\nDF2_CF_col = vectorizer(DF_CF_col, 'features', 'vectors', 2000, 5)\n\n#dimension reduction of dataset\nDF3_CF_col = getPCA(DF2_CF_col, 'vectors', 2)"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"markdown","source":["#### 6.6.1 Visualization using Matplotlib"],"metadata":{}},{"cell_type":"code","source":["#x and y values of data in list form\nX_colCF = DF3_CF_col.rdd.map(lambda p: p['pcaFeatures'][0]).collect()\nY_colCF = DF3_CF_col.rdd.map(lambda p: p['pcaFeatures'][1]).collect()\n\n#organization names in a list\ncolCF = DF3_CF_col.rdd.map(lambda p: p[0]).collect()"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["fig6, axs6 = plt.subplots()\nplt.title(\"Co-Following on machine-learning organizations and minecraft game add-ons companies\")\naxs6.scatter(X_colCF,Y_colCF)\nfor i, txt in enumerate(colCF):\n    axs6.annotate(txt, (X_colCF[i],Y_colCF[i]))\nfig6.set_size_inches(18.5, 12, forward=True)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":["####6.6.2 Plot Interpretation\n+ This graph shows significant result among others.\n+ `essentials`, `GlowstoneMC`, `SpigotMC`, `Bukkit`, `MinecraftForge`, `PocketMine`, `overviewer`, `MightyPirates` are all clustered together as they all have minecraft add-on projects. While machine learning organizations such as `scikit-learn`, `tensorflow`, `theano` grouped at the bottom."],"metadata":{}},{"cell_type":"markdown","source":["##7. Interpretation of Result\n+ Notes on Classification Result\n  + As co-following and co-membership data get very sparse for less number of members per organization. Therefore, we deliberately selected 3 organization who has average 600 members in each of them. \n  + To make the classification run quickly, we selected the vector size of 1000 and (minDF=5) *(see the function 'vectorizer' for more detail)*\n  + The average classification accuracy showed almost similar result in co-membership and co-following. Moreover, We are not limiting the co-following data or co-membership data by any means as the author has done in main paper for twitter. We used the exhaustive list of co-following and co-members.\n  + The accuracy is not measured for general sample due to time constraint of project.\n+ Comparison of Classification and PCA Result\n  + Both co-membership and co-followings features given nearly similar classification accuracy for 3 organization selected. However, co-membership shown relatively high accuracy than co-following which suggests that co-membership would be prefered choice over co-following for PCA analysis.\n  + On the contrary, PCA showed more interpretable result with co-following than co-membership which might be due to fact that co-following relation is distributed evenly across all user but organization-user membership relation is not evenly distributed. This is because of some of the organizations do not allow members' list to be viewed publically."],"metadata":{}},{"cell_type":"markdown","source":["##8. Conclusions:\n+ Co-following similarity is more significant than coMembership similarity since the membership doesn't reflect the interest of the user because user him/herself cannot join any organization as a member. Moreover, organizations in GitHub can decide whether they want their memebrs to be publicly visible or not. Which makes the dataset not very accurate.\n+ While followship is totally a user's call. That's why following relationship shows more significant result in similarity measurement.\n+ Top organizations are not necessarily be similar to each other and it is reflected in the result here. However, the curated list (aka Collections) of github repositories shows more similar-to-real-world result of similarity.\n+ Since an organization could have more than one repositories and each being very different to each other. The members involved in certain project attracts certain user group as following and others have very different followings which may result in less significant similarity result.\n+ Organizations similarity needed to be cross validated which will require organizations' business insight and how organizations differs in term of their tendency to use propriotery software architectures and frameworks as well as their OpenSource business philosophy and agendas. This kind of in-depth knowledge might help to interpret the result more meaningfully. However, it get restricted by scope and time limit of project.  \n\n##9. Future Work:\n+ I would like to extend this project to check the co-following nature in Repositories as well. Because  users can also watch or fork any open repository of their choice. This users' watching and forking other repositories might results into finding more similar repositories which can be recommended to similar users.\n+ Moreover, repositories similarity could be evaluated in terms of their programming language, main objective of project and trending topics."],"metadata":{}},{"cell_type":"markdown","source":["##10. REFERENCES\n1. Venkata Rama Kiran Garimella and Ingmar Weber. 2014. Co-following on twitter. In Proceedings of the 25th ACM conference on Hypertext and social media (HT '14). ACM, New York, NY, USA, 249-254. DOI=http://dx.doi.org/10.1145/2631775.2631820\n2. Gousios, G. (2013). The GHTorrent dataset and tool suite. In Proceedings of the 10th Working Conference on Mining Software Repositories (pp. 233–236). Retrieved from http://www.gousios.gr/bibliography/G13.html\n3. GHimpact. http://www.gh-impact.com/about/\n4. GitHub Collecitons : https://github.com/collections\n5. Georgios Gousios and Diomidis Spinellis. 2012. GHTorrent: GitHub's data from a firehose. In Proceedings of the 9th IEEE Working Conference on Mining Software Repositories (MSR '12). IEEE Press, Piscataway, NJ, USA, 12-21.\n6. J.   Davis   and   M.   Goadrich, “The   relationship between   Precision-Recall   and   ROC   curves,”in Proceedings of the 23rd international conference on Machine learning  -ICML ’06, 2006, pp. 233–240.\n7. E. Kalliamvakou, G. Gousios, and K. Blincoe, “The Promises and Perils of Mining GitHub,”Proc. 11th, pp. 92–101, 2014."],"metadata":{}}],"metadata":{"name":"MIE1512_FINAL_DATABRICKS_JAYPATEL","notebookId":3206093881563512},"nbformat":4,"nbformat_minor":0}
